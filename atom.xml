<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>樱花飘落之时</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yhcheer.com/"/>
  <updated>2019-05-12T06:09:17.579Z</updated>
  <id>http://yhcheer.com/</id>
  
  <author>
    <name>yh</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【置顶】关于我，活成自己想要的样子</title>
    <link href="http://yhcheer.com/2048/11/11/hello-world/"/>
    <id>http://yhcheer.com/2048/11/11/hello-world/</id>
    <published>2048-11-11T03:11:11.000Z</published>
    <updated>2019-05-12T06:09:17.579Z</updated>
    
    <content type="html"><![CDATA[<p>有生之年，我的博客终于翻新了。</p><a id="more"></a><p>一般总结好的博文，我才会放在我的”樱飘”博客里，然后通过外链接出去（为了整洁），没做好整理的就零散落在其他博客或者在某个旧电脑硬盘，你可能无缘看到。</p><p>本科HIT，为了和酒神成为校友，读了ZJU的研，然后顺便转个行。想要找我的，联系方式在最下方。</p><p>无敌热爱图形学，即将入职网易游戏互娱事业群实习，想成为一名技术美术（Technical Artist，TA），实现自己喜欢的效果，渲染出自己喜欢的世界。</p><p>喜欢摄影和绘画，但作品很少；想拍人像，但缺个模特。</p><p><img src="//r.photo.store.qq.com/psb?/2a0ad7a8-f750-4d2b-8f4e-b4fc75087129/VnPjISITk0Vdsj264GzTrzHDpmSBqCDaSH4bklD61qM!/r/dL8AAAAAAAAA&amp;bo=AAWWAwAFlgMRCT4!&amp;rf=viewer_4_yake_qzoneimgout.png" alt=""></p><p><img src="//r.photo.store.qq.com/psb?/2a0ad7a8-f750-4d2b-8f4e-b4fc75087129/iYMcGsmMDYfY9hDb8QdNLc3.YJJG5feHArMGjBmAFMc!/r/dL4AAAAAAAAA&amp;bo=AAXAAwAFwAMRCT4!&amp;rf=viewer_4_yake_qzoneimgout.png" alt=""></p><p><img src="//r.photo.store.qq.com/psb?/2a0ad7a8-f750-4d2b-8f4e-b4fc75087129/cDiFgHq8mMhlScJPxDaVx9TYl.NXfQgOsk7TVM*BF2I!/r/dL8AAAAAAAAA&amp;bo=AAUkBAAFJAQRGS4!&amp;rf=viewer_4_yake_qzoneimgout.png" alt=""></p><p><img src="//r.photo.store.qq.com/psb?/2a0ad7a8-f750-4d2b-8f4e-b4fc75087129/3DKBb1UHjn8rAXa9WZmFrJ8bYBMmuD4bYAQXLPwYPp8!/r/dDcBAAAAAAAA&amp;bo=VAY4BFQGOAQRCT4!&amp;rf=viewer_4_yake_qzoneimgout.png" alt=""></p><p><img src="//r.photo.store.qq.com/psb?/2a0ad7a8-f750-4d2b-8f4e-b4fc75087129/ndpgI2uFNTclpWProTToNz*K1W1sIQKpAQ9k4ZYL1PM!/r/dDMBAAAAAAAA&amp;bo=AAXAAwAFwAMRCT4!&amp;rf=viewer_4_yake_qzoneimgout.png" alt=""></p><p><strong>知乎：</strong></p><p>为了监督自己好好写文章换的真名：<a href="https://www.zhihu.com/people/cheerup/activities" target="_blank" rel="noopener">个人主页</a>（最近动态几乎都是cg相关）</p><p><strong>两个知乎专栏：</strong></p><p>计算机图形学（艰难更新）：<a href="https://zhuanlan.zhihu.com/romance" target="_blank" rel="noopener">风花雪月</a></p><p>计算机视觉（有生之年）：<a href="https://zhuanlan.zhihu.com/justcv" target="_blank" rel="noopener">JustCV</a></p><p><strong>CSDN：</strong></p><p>零碎博文放置处之一：<a href="https://blog.csdn.net/u013611405" target="_blank" rel="noopener">幻想郷こうまかん</a></p><p><strong>QQ：</strong></p><p>一般用来打游戏/交流游戏：406943342</p><p><strong>微博：</strong></p><p>乌烟瘴气/互喷：<a href="https://weibo.com/1972246445/profile?topnav=1&amp;wvr=6&amp;is_all=1#_rnd1551374727735" target="_blank" rel="noopener">@yh_cheer</a></p><p><strong>网易云音乐：</strong></p><p>什么歌都听：yhcheer</p><p><strong>Steam ID：</strong></p><p>白茶清欢无别事</p><p><strong>邮箱：</strong></p><p>会很认真看：<a href="mailto:yhcheer@zju.edu.cn" target="_blank" rel="noopener">yhcheer@zju.edu.cn</a></p><hr><p>赞美生命，热爱生活。</p><p>独立思考，谨言慎行。</p><p>优先级依次递减。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有生之年，我的博客终于翻新了。&lt;/p&gt;
    
    </summary>
    
      <category term="EGO" scheme="http://yhcheer.com/categories/EGO/"/>
    
    
      <category term="心情" scheme="http://yhcheer.com/tags/%E5%BF%83%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>刷题陪跑</title>
    <link href="http://yhcheer.com/2019/07/19/normal-heart/"/>
    <id>http://yhcheer.com/2019/07/19/normal-heart/</id>
    <published>2019-07-19T14:18:00.000Z</published>
    <updated>2019-07-19T14:19:00.875Z</updated>
    
    <content type="html"><![CDATA[<p>这里是摘要。</p><a id="more"></a><table><thead><tr><th></th><th>yh</th><th>麦朵</th></tr></thead><tbody><tr><td>2019.07.19</td><td><a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">3. 无重复字符的最长子串</a></td><td>130. 被围绕的区域</td></tr><tr><td></td><td></td><td>牛牛的闹钟</td></tr><tr><td></td><td></td><td>牛牛的背包问题</td></tr><tr><td>2019.07.20</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里是摘要。&lt;/p&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="http://yhcheer.com/categories/Algorithm/"/>
    
    
      <category term="算法" scheme="http://yhcheer.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>《剑指Offer》刷题记录</title>
    <link href="http://yhcheer.com/2019/03/16/jianzhi-offer/"/>
    <id>http://yhcheer.com/2019/03/16/jianzhi-offer/</id>
    <published>2019-03-15T17:00:00.000Z</published>
    <updated>2019-03-29T14:38:46.595Z</updated>
    
    <content type="html"><![CDATA[<p>开一个新坑，并且一定会填完！这篇文章记录剑指Offer题解记录，<del>不求最优解，但求最简解</del>。私以为：好记，并且在面试或笔试中能快速回忆起来或写出来的算法，才是好算法！</p><a id="more"></a><p>题号按第二版原书为准，与牛客网的顺序不同，请善用Ctrl+F。</p><table><thead><tr><th>题解</th><th>考点</th></tr></thead><tbody><tr><td>————————————————————————————————–</td><td>———————————————</td></tr><tr><td>面试题44：数字序列中的某一位的数字</td><td></td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88587899" target="_blank" rel="noopener">面试题45：把数组排成最小的数</a></td><td>排序 大数问题</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88588798" target="_blank" rel="noopener">面试题46：把数字翻译成字符串</a></td><td>递归</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88595582" target="_blank" rel="noopener">面试题47：礼物的最大价值</a></td><td>递归 动态规划</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88619171" target="_blank" rel="noopener">面试题48：最长不含重复字符的子字符串</a></td><td>递归 动态规划</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88631226" target="_blank" rel="noopener">面试题49：丑数</a></td><td>数学题</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88637508" target="_blank" rel="noopener">面试题50：第一次只出现一次的字符</a></td><td>哈希表</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88644831" target="_blank" rel="noopener">面试题51：数组中的逆序对</a></td><td>归并排序</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88793428" target="_blank" rel="noopener">面试题52：两个链表的第一个公共结点</a></td><td>链表</td></tr><tr><td>第六章</td><td></td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88827729" target="_blank" rel="noopener">面试题53：在排序数组中查找数字</a></td><td>查找</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88828867" target="_blank" rel="noopener">面试题54：二叉搜索树的第K小节点</a></td><td>树</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88829190" target="_blank" rel="noopener">面试题55：二叉树的深度</a></td><td>树 递归</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88891682" target="_blank" rel="noopener">面试题55：平衡二叉树</a></td><td>树 递归</td></tr><tr><td><a href="https://blog.csdn.net/u013611405/article/details/88895524" target="_blank" rel="noopener">面试题56：数组中只出现一次的数字</a></td><td>异或</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td></tr></tbody></table><p>第六七章这两天施工补上。</p><p>第五章以前的部分，待我刷第二轮的时候补上。</p><p>祝好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开一个新坑，并且一定会填完！这篇文章记录剑指Offer题解记录，&lt;del&gt;不求最优解，但求最简解&lt;/del&gt;。私以为：好记，并且在面试或笔试中能快速回忆起来或写出来的算法，才是好算法！&lt;/p&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="http://yhcheer.com/categories/Algorithm/"/>
    
    
      <category term="算法" scheme="http://yhcheer.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="C++" scheme="http://yhcheer.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Texture Maps Explained - PBR Workflow</title>
    <link href="http://yhcheer.com/2019/03/12/pbr-workflow/"/>
    <id>http://yhcheer.com/2019/03/12/pbr-workflow/</id>
    <published>2019-03-12T15:09:00.000Z</published>
    <updated>2019-03-12T16:36:54.509Z</updated>
    
    <content type="html"><![CDATA[<p>An interesting PBR tutorial video from <a href="https://www.youtube.com/watch?v=PjGCtnEDDeU" target="_blank" rel="noopener">youtube</a>.<br><a id="more"></a></p><h3 id="Base-Color-Albedo-Diffuse-map"><a href="#Base-Color-Albedo-Diffuse-map" class="headerlink" title="Base Color / Albedo / Diffuse map"></a>Base Color / Albedo / Diffuse map</h3><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/rjcG184Hh8kN1UwfDEZYl*JpsJ7B1Yk47Rcf98VLvb0!/r/dL8AAAAAAAAA&amp;bo=IgS9ASIEvQEDORw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="Base Color Texture"></p><p>Base Color Texture</p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/Fqa8a033FbLVE7sYeUx4QLgCIdYVnKRJFXV179RVU7o!/r/dLYAAAAAAAAA&amp;bo=AATEAQAExAEDORw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552400122698"> </p><p>In the picture below, the left one has base color texture but the right one hasn’t</p><h3 id="Metallic"><a href="#Metallic" class="headerlink" title="Metallic"></a>Metallic</h3><p><strong>Metallic map is ability of your surface to reflect the image around it.</strong></p><p><strong>so it is reflecting the environment that may be near it.</strong></p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/fJ.ItDNdENuoY6vrEyHfgVbeez8F.fadUhX4K1MhMss!/r/dLYAAAAAAAAA&amp;bo=LwS8AS8EvAEDGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="Metallic Texture"> </p><p>completely black Metallic Texture</p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/v*IXQ4EF9PWG*fOPevkXd3mCiTuCo*SrNsujbDXYnEk!/r/dMAAAAAAAAAA&amp;bo=JwS6AScEugEDORw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552400261053"></p><p>the left Metallic map is whole black ,so there’s no “metalness” to the surface </p><p>and the right one is completely white , so it’s completely metallic. (In the picture may be a sun in the right hand side.)</p><h3 id="Roughness"><a href="#Roughness" class="headerlink" title="Roughness"></a>Roughness</h3><p><strong>the ability of your surface to reflect or absorb white light.</strong></p><ul><li><ul><li>the more white your map is: the less light it will be bouncing back and it will be absorbing </li></ul></li><li><ul><li>the more dark or black that your texture surface: the more light it will be reflecting and the less that you’ll be taking in</li><li>In a word, <strong>white is rough</strong>, on the contrary, <strong>black is very smooth</strong>.</li></ul></li></ul><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/M5IrSJwTnWMmRZgqs0M23OKsC0SBsdeUxuYJgjLphn8!/r/dMIAAAAAAAAA&amp;bo=EwTBARMEwQEDKQw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="Roughness Texture"> </p><p>Roughness Texture</p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/8zC0t0hM2qOkH8fbNpXTk7p1aCyhKyUE2HNtMJPyTo8!/r/dFQBAAAAAAAA&amp;bo=MwTBATMEwQEDORw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552402389855"></p><p>In the picture the right one use the Metallic map &amp; roughness map, so it absorb some light. It’s not very metal and it’s not very rough, it’s balanced.</p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/y.cmvjGyk.7KH0q8lwzX13XoRt5XM8RPBJiIwBAzd.c!/r/dDYBAAAAAAAA&amp;bo=LwS*AS8EvwEDORw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552403800669"></p><p>In extreme situation, very white metallic map + very black roughness map. (the right one)</p><h3 id="Normals"><a href="#Normals" class="headerlink" title="Normals"></a>Normals</h3><p><strong>essentially determines the illusion of depth and features</strong></p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/nFqXOthBhW7lQGbcdISFuSVKK15q*thzZAdi0GNOwrs!/r/dL4AAAAAAAAA&amp;bo=uwWAAjsGuAIDSdc!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552404220012"> </p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/GQukfeo5Le13*wao9JgbJGciSibBxYq0tLZS9vuC5L4!/r/dLwAAAAAAAAA&amp;bo=yQWAAkQGtQIDSdc!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552404270070"> </p><p>on the left have Normal Texture map, looks like 3d and have a lot of details. </p><h3 id="Height"><a href="#Height" class="headerlink" title="Height"></a>Height</h3><p><strong>fake depth or essentially height on your map</strong></p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/NupGrt1.74AYPzbKx52N7s*xGXZIk0SLUSaUWT*tYoM!/r/dL4AAAAAAAAA&amp;bo=hgWAAh4GxQIDKdI!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552404492444"></p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/yUpmsH5IXbvAhnTcMa.9o9RSfmYarrdh6c6oc3YPOaE!/r/dFIBAAAAAAAA&amp;bo=pgWAAi4GvAIDSds!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552404820506"></p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/XVTorxoFHkzVPe9xJmeAI1k1b0ngkceTfj*HdetRS.0!/r/dDcBAAAAAAAA&amp;bo=mQWAAi8GwwIDSZo!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552404993149"></p><p>the height map is giving us more depth on the left side of our cube</p><p>the right one has height map</p><h3 id="Ambient-Occlusion"><a href="#Ambient-Occlusion" class="headerlink" title="Ambient Occlusion"></a>Ambient Occlusion</h3><p><strong>determines which areas are inherently darkened to simulate shadows</strong></p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/j5CjXHAZP36DEvdgj37WEpk1mvdN2yP.Pm4XH9klzJY!/r/dFIBAAAAAAAA&amp;bo=rwWAAi4GuAIDOaY!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552405777944"></p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/FHeDsCpxXgU1COSuSXgaZI*sWZLQCA7hg.mVuQsXnnI!/r/dLYAAAAAAAAA&amp;bo=wAWAAjMGsgIDSa4!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1552406559075"></p><p>left side have AO applied on and we’ve got darker crevices as if there’s shadows there</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;An interesting PBR tutorial video from &lt;a href=&quot;https://www.youtube.com/watch?v=PjGCtnEDDeU&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;youtube&lt;/a&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="CG" scheme="http://yhcheer.com/categories/CG/"/>
    
    
      <category term="计算机图形学" scheme="http://yhcheer.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
      <category term="PBR" scheme="http://yhcheer.com/tags/PBR/"/>
    
  </entry>
  
  <entry>
    <title>清华大学《计算机图形学基础》提炼总结</title>
    <link href="http://yhcheer.com/2018/12/21/graph-note/"/>
    <id>http://yhcheer.com/2018/12/21/graph-note/</id>
    <published>2018-12-21T07:30:00.000Z</published>
    <updated>2019-03-12T17:20:28.068Z</updated>
    
    <content type="html"><![CDATA[<p>由清华大学胡事民教授的<strong>《计算机图形学基础》</strong>课程教案中，个人提取出的一些<strong>学习笔记</strong>。<strong>主要内容</strong>包括：图形学简介、颜色模型、图像基本知识、Phong光照模型、视图模型变换、材质反射属性模型BRDF、光线跟踪、Bezier曲线曲面、B样条曲线曲面、网格、光线跟踪加速算法、纹理、阴影生成等。本文主要以QA的形式进行总结。</p><a id="more"></a><p>课程主页：<a href="https://cg.cs.tsinghua.edu.cn/course/resource.htm" target="_blank" rel="noopener">https://cg.cs.tsinghua.edu.cn/course/resource.htm</a></p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p><a href="http://yhcheer.com/2018/12/21/graph-note-01/">第一讲 图形学简介、历史</a></p><p><a href="http://yhcheer.com/2018/12/21/graph-note-02/">第二讲 颜色模型、图像基本知识、Phong光照模型</a></p><p><a href="http://yhcheer.com/2018/12/27/graph-note-03/">第三讲 材质反射属性模型BRDF</a></p><p>施工施工！</p><p>之前写好的三篇需要再排版！强迫症患者！</p><p>过段时间再po出来。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由清华大学胡事民教授的&lt;strong&gt;《计算机图形学基础》&lt;/strong&gt;课程教案中，个人提取出的一些&lt;strong&gt;学习笔记&lt;/strong&gt;。&lt;strong&gt;主要内容&lt;/strong&gt;包括：图形学简介、颜色模型、图像基本知识、Phong光照模型、视图模型变换、材质反射属性模型BRDF、光线跟踪、Bezier曲线曲面、B样条曲线曲面、网格、光线跟踪加速算法、纹理、阴影生成等。本文主要以QA的形式进行总结。&lt;/p&gt;
    
    </summary>
    
      <category term="CG" scheme="http://yhcheer.com/categories/CG/"/>
    
    
      <category term="计算机图形学" scheme="http://yhcheer.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
      <category term="清华大学" scheme="http://yhcheer.com/tags/%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop集群下的MapReduce实现</title>
    <link href="http://yhcheer.com/2018/11/01/hadoop-mapreduce/"/>
    <id>http://yhcheer.com/2018/11/01/hadoop-mapreduce/</id>
    <published>2018-11-01T07:15:00.000Z</published>
    <updated>2019-03-12T15:07:26.540Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章记录<strong>MapReduce</strong>的编程和实现，语言用的是<strong>JAVA</strong>。</p><p>搭环境的时候我们用hadoop自带的<strong>wordcount</strong> example来当作HelloWorld测试环境是否成功搭建，现在我们手撸MapReduce，同样从<strong>HelloWorld</strong>开始。</p><a id="more"></a><p>导包，这里我新建maven工程，把包交由maven管理，记得改成你安装的hadoop版本和jdk版本。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hdp<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>wordcount<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">hadoopVersion</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">hadoopVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Hadoop start --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoopVersion&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoopVersion&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoopVersion&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoopVersion&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Hadoop end --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p>一个MapReduce由三部分组成。新建如下三个类，最后打成jar包。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * k3：单词 -&gt; v3：记录个数列表(1,1,1...)</span></span><br><span class="line"><span class="comment"> * k4：单词 -&gt; v4：个数统计</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key3, Iterable&lt;IntWritable&gt; value3, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * context 是Reduce的上下文</span></span><br><span class="line"><span class="comment">         * 上文：mapper</span></span><br><span class="line"><span class="comment">         * 下文：HDFS</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(IntWritable val : value3)&#123;</span><br><span class="line">            sum += val.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        result.set(sum);</span><br><span class="line">        context.write(key3, result); <span class="comment">//k3 == k4，输出k4 -&gt; v4</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  k1: 偏移量 -&gt; v1：数据（每行）</span></span><br><span class="line"><span class="comment"> *  k2：数据（每个单词） -&gt; v2：记单词个数1</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key1, Text value1, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        Context上下文</span></span><br><span class="line"><span class="comment">        上文：HDFS</span></span><br><span class="line"><span class="comment">        下文：Reduce</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        String data = value1.toString();</span><br><span class="line"></span><br><span class="line">        StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(data);<span class="comment">//分解字符串</span></span><br><span class="line">        <span class="keyword">while</span> (itr.hasMoreTokens())&#123;</span><br><span class="line">            word.set(itr.nextToken());<span class="comment">//更新k2</span></span><br><span class="line">            context.write(word, one); <span class="comment">//利用上下文进行输出 k2 -&gt; v2</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(otherArgs.length != <span class="number">2</span>)&#123;</span><br><span class="line">            System.err.println(<span class="string">"Usage: wordcount &lt;in&gt; &lt;out&gt;"</span>);</span><br><span class="line">            System.exit(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//创建一个任务，并指定入口</span></span><br><span class="line">        Job job = <span class="keyword">new</span> Job(conf, <span class="string">"word count"</span>);</span><br><span class="line">        job.setJarByClass(WordCount.class);</span><br><span class="line">        <span class="comment">//指定任务map，及map输出的类型&lt;k2,v2&gt;</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);<span class="comment">//k2</span></span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);<span class="comment">//v2</span></span><br><span class="line">        <span class="comment">//指定任务reduce</span></span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);<span class="comment">//优化，相当于本地的Reduce</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);<span class="comment">//k4</span></span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);<span class="comment">//v4</span></span><br><span class="line">        <span class="comment">//输入输出路径</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));<span class="comment">//注意导新API的包</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>]));</span><br><span class="line">        <span class="comment">//执行任务</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>参考：</p><p>Hadoop所需jar包：<a href="https://blog.csdn.net/qq_33813365/article/details/70214484" target="_blank" rel="noopener">https://blog.csdn.net/qq_33813365/article/details/70214484</a></p><p>TopK：<a href="https://www.imooc.com/article/details/id/20699#comment" target="_blank" rel="noopener">https://www.imooc.com/article/details/id/20699#comment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章记录&lt;strong&gt;MapReduce&lt;/strong&gt;的编程和实现，语言用的是&lt;strong&gt;JAVA&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;搭环境的时候我们用hadoop自带的&lt;strong&gt;wordcount&lt;/strong&gt; example来当作HelloWorld测试环境是否成功搭建，现在我们手撸MapReduce，同样从&lt;strong&gt;HelloWorld&lt;/strong&gt;开始。&lt;/p&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://yhcheer.com/categories/BigData/"/>
    
    
      <category term="Hadoop" scheme="http://yhcheer.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="http://yhcheer.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="MapReduce" scheme="http://yhcheer.com/tags/MapReduce/"/>
    
      <category term="Java" scheme="http://yhcheer.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>用阿里云搭建Hadoop集群</title>
    <link href="http://yhcheer.com/2018/10/23/hadoop-from-zero/"/>
    <id>http://yhcheer.com/2018/10/23/hadoop-from-zero/</id>
    <published>2018-10-23T14:10:33.000Z</published>
    <updated>2019-03-15T17:06:55.421Z</updated>
    
    <content type="html"><![CDATA[<p>随着云服务器的飞速发展，伴随着<strong>云平台</strong>的各种巨大优势所在，在云上<strong>配置Hadoop集群</strong>似乎是最正确的选择。</p><p>这次用了3台<strong>阿里云ECS服务器</strong>（学生机）来完成这次的环境搭建。由于是三个不同的账号（每个账号的学生优惠只能有一个实例），所以走的是<strong>外网IP通道</strong>，和网上大部分教程有一定的区别。</p><a id="more"></a><h2 id="一、HADOOP集群搭建"><a href="#一、HADOOP集群搭建" class="headerlink" title="一、HADOOP集群搭建"></a>一、HADOOP集群搭建</h2><h3 id="1-服务器准备"><a href="#1-服务器准备" class="headerlink" title="1 服务器准备"></a>1 服务器准备</h3><p>阿里云ESC服务器 1 vCPU 2 GB (I/O优化) ecs.n4.small 1Mbps </p><p>默认系统选的Centos 7.4 64bit</p><p>阿里云已经导入学信网系统，学生优惠9.5/月，你值得拥有</p><h3 id="2-网络环境准备"><a href="#2-网络环境准备" class="headerlink" title="2 网络环境准备"></a>2 网络环境准备</h3><p>阿里云自动分配公网IP和私网IP，记住即可</p><p>阿里云的安全组设置默认只开启22端口用于SSH，以及3389端口用于远程登录</p><p>对于搭hadoop集群的我们肯定是不够的，如果你不太了解hadoop的端口配置，就到阿里云控制台上在你的实例的安全组上，添加如下一步到位的规则：</p><p>规则方向-出方向和入方向，协议类型-全部，授权类型-地址段访问，授权对象-0.0.0.0/0</p><p>当然这样是很不安全的，但对于新手来说，省了很多麻烦，建议后期手动修改安全组设置。</p><h3 id="3-服务器系统设置"><a href="#3-服务器系统设置" class="headerlink" title="3 服务器系统设置"></a>3 服务器系统设置</h3><p>3.1 添加HADOOP用户</p><p><del>不添加。</del>我是用root用户来搭建。</p><p><strong>PS. 我第一次搭的时候，使用的是HADOOP用户，但尝试了各种方法，都无法成功在HADOOP用户下进行ssh免密，导致最后跑集群的时候出错。</strong></p><p>3.2 分配root权限</p><p><del>root用户无需再分配</del></p><p>3.3 同步时间</p><p><del>阿里云无需同步时间</del></p><p>3.4 设置主机名（重要）</p><p><code>sudo nano /etc/hostname</code></p><p>我习惯用nano，没有的话装一下<code>yum install nano</code></p><p>如果是自己的主机的话直接在“阿里云实例设置-修改信息-重启实例”也能改。</p><p>三台机器主机名分别设y为：</p><ul><li>hdp-node-01</li><li>hdp-node-02</li><li>hdp-node-03</li></ul><p>3.5 域名映射（重要）</p><p>将一些常用的网址域名与其对应的IP地址建立一个关联“数据库”</p><p><code>sudo nano /etc/hosts</code></p><p>在<strong>文件最下方</strong>添加如下信息，我这里用外网IP。如果是在同一个账号下的三个实例，可以选择走内网IP通道，速度上应该会快很多。</p><ul><li>本机<strong>内网IP</strong> hdp-node-01</li><li>从机外网IP hdp-node-02</li><li>从机外网IP hdp-node-03</li></ul><p><strong>PS. 这里一开始我用的都是外网IP，最后报错，原因大概是阿里云的外网IP冲突问题，因此改为内网IP。注意只有主机的hosts配置的第一个节点（本机）是内网IP，并且另外几台从机的hosts配置都是使用外网IP不用改变。</strong></p><p>3.6 配置ssh免密登录（重要）</p><p><code>ssh-keygen</code>利用ssh-keygen生成密钥</p><p><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code>加入到授权，可以免密ssh自己</p><p>把密匙公钥发送给别的节点，已达到免密的效果，发送时要输入对面节点的密码</p><p><code>scp ~/.ssh/id_rsa.pub root@hdp-node-02:~/.ssh</code>将id_rsa.pub拷贝到hdp-node-02节点上</p><p><code>scp ~/.ssh/id_rsa.pub root@hdp-node-03:~/.ssh</code>将id_rsa.pub拷贝到hdp-node-03节点上</p><p><strong>换到hdp-node-02和hdp-node-03上操作：</strong></p><p><code>cat ~/.ssh/id_rsa.pub &gt;&gt;~/.ssh/authorized_keys</code>把刚才的密匙加入到授权</p><p><strong>回到hdp-node-01上操作：</strong></p><p>测试<code>ssh hdp-node-02</code>成功！如果还需要输入密码则免密失败。</p><p><code>logout</code>退出刚才的ssh</p><p><strong>PS. 后来调试bug的时候，我把所有节点相互之间的ssh免密也实现了，理论上应该只需要主机ssh所有从机即可。</strong></p><p>3.7 配置防火墙</p><p><code>systemctl stop firewalld</code>关闭centos7防火墙</p><h3 id="4-JDK环境安装"><a href="#4-JDK环境安装" class="headerlink" title="4 JDK环境安装"></a>4 JDK环境安装</h3><p>JDK是HADOOP的必要环境</p><p>下载jdk1.8，可以用wget方法，我就用FileZilla传上去再解压好了</p><p>放到/usr文件夹下，解压</p><p><code>tar -zxf jdk-8u191-linux-x64.tar.gz</code></p><p>重命名一下文件夹名字为java（非必须）</p><p><code>mv jdk1.8.0_191 java</code></p><p>配置环境变量<code>nano /etc/profile</code>，在<strong>文件最下方</strong>添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>生效环境<code>source /etc/profile</code></p><p>测试<code>java -version</code>，成功则显示版本号</p><p>重复以上步骤在每个从机上执行</p><h3 id="5-hadoop安装和部署"><a href="#5-hadoop安装和部署" class="headerlink" title="5 hadoop安装和部署"></a>5 hadoop安装和部署</h3><p>下载、解压。同上。</p><p>重命名一下（非必须）</p><p><code>mv hadoop2.7.7 hadoop</code></p><p><strong>PS. 我这里使用的是hadoop 2.7.7版本，注意hadoop 3与hadoop 2的改动比较大，包括端口、命令、以及一些配置等，注意区别。</strong></p><p>配置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java</span><br><span class="line">export PATH=PATH:$JAVA_HOME/bin</span><br><span class="line">export HADOOP_HOME=/usr/hadoop</span><br><span class="line">export PATH=PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><p>生效环境<code>source /etc/profile</code></p><p>测试<code>hadoop version</code>显示版本号，则成功</p><h3 id="6-开始部署hadoop"><a href="#6-开始部署hadoop" class="headerlink" title="6 开始部署hadoop"></a>6 开始部署hadoop</h3><p>进入hadoop安装目录下的<strong>子目录</strong>/etc/hadoop</p><p><code>cd /usr/hadoop/etc/hadoop</code></p><ul><li>配置JAVA路径</li></ul><p><code>nano hadoop-env.sh</code>找到下面写有JAVA_HOME的地方，修改JAVA路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.</span><br><span class="line">export JAVA_HOME=/usr/java</span><br></pre></td></tr></table></figure><ul><li>配置core-site.xml</li></ul><p>集群全局参数，用于定义系统级别的参数，如HDFS  URL、Hadoop的临时目录等</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hdp-node-01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>序号</th><th>参数名</th><th>默认值</th><th>参数解释</th></tr></thead><tbody><tr><td>1</td><td>fs.defaultFS</td><td>file:///</td><td>文件系统主机和端口</td></tr><tr><td>2</td><td>io.file.buffer.size</td><td>4096</td><td>流文件的缓冲区大小</td></tr><tr><td>3</td><td>hadoop.tmp.dir</td><td>/tmp/hadoop-${user.name}</td><td>临时文件夹</td></tr></tbody></table><ul><li>配置hdfs-site.xml</li></ul><p>HDFS参数，如名称节点和数据节点的存放位置、文件副本的个数、文件读取权限等</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdp-node-02:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.https-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdp-node-02:50091<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>序号</th><th>参数名</th><th>默认值</th><th>参数解释</th></tr></thead><tbody><tr><td>1</td><td>dfs.namenode.secondary.http-address</td><td>0.0.0.0:50090</td><td>定义HDFS对应的HTTP服务器地址和端口</td></tr><tr><td>2</td><td>dfs.namenode.name.dir</td><td>file://${hadoop.tmp.dir}/dfs/name</td><td>定义DFS的名称节点在本地文件系统的位置</td></tr><tr><td>3</td><td>dfs.datanode.data.dir</td><td>file://${hadoop.tmp.dir}/dfs/data</td><td>定义DFS数据节点存储数据块时存储在本地文件系统的位置</td></tr><tr><td>4</td><td>dfs.replication</td><td>3</td><td>缺省的块复制数量</td></tr><tr><td>5</td><td>dfs.webhdfs.enabled</td><td>true</td><td>是否通过http协议读取hdfs文件，如果选是，则集群安全性较差</td></tr></tbody></table><ul><li>配置mapred-site.xml</li></ul><p>Mapreduce参数，包括JobHistory Server和应用程序参数两部分，如reduce任务的默认个数、任务所能够使用内存的默认上下限等</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>序号</th><th>参数名</th><th>默认值</th><th>参数解释</th></tr></thead><tbody><tr><td>1</td><td>mapreduce.framework.name</td><td>local</td><td>取值local、classic或yarn其中之一，如果不是yarn，则不会使用YARN集群来实现资源的分配</td></tr><tr><td>2</td><td>mapreduce.jobhistory.address</td><td>0.0.0.0:10020</td><td>定义历史服务器的地址和端口，通过历史服务器查看已经运行完的Mapreduce作业记录</td></tr><tr><td>3</td><td>mapreduce.jobhistory.webapp.address</td><td>0.0.0.0:19888</td><td>定义历史服务器web应用访问的地址和端口</td></tr></tbody></table><ul><li>配置yarn-site.xml</li></ul><p>集群资源管理系统参数，配置 ResourceManager，NodeManager 的通信端口，web监控端口等</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- RM的hostname --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定Yarn的老大(ResourceManager)的地址--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdp-node-01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>序号</th><th>参数名</th><th>默认值</th><th>参数解释</th></tr></thead><tbody><tr><td>1</td><td>yarn.resourcemanager.address</td><td>0.0.0.0:8032</td><td>ResourceManager 提供给客户端访问的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等</td></tr><tr><td>2</td><td>yarn.resourcemanager.scheduler.address</td><td>0.0.0.0:8030</td><td>ResourceManager提供给ApplicationMaster的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等</td></tr><tr><td>3</td><td>yarn.resourcemanager.resource-tracker.address</td><td>0.0.0.0:8031</td><td>ResourceManager 提供给NodeManager的地址。NodeManager通过该地址向RM汇报心跳，领取任务等</td></tr><tr><td>4</td><td>yarn.resourcemanager.admin.address</td><td>0.0.0.0:8033</td><td>ResourceManager 提供给管理员的访问地址。管理员通过该地址向RM发送管理命令等。</td></tr><tr><td>5</td><td>yarn.resourcemanager.webapp.address</td><td>0.0.0.0:8088</td><td>ResourceManager对web 服务提供地址。用户可通过该地址在浏览器中查看集群各类信息</td></tr><tr><td>6</td><td>yarn.nodemanager.aux-services</td><td></td><td>通过该配置项，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的，这样就可以在NodeManager上扩展自己的服务。</td></tr></tbody></table><ul><li>配置slaves</li></ul><p><code>nano slaves</code></p><p>hdp-node-02<br>hdp-node-03</p><ul><li>配置masters</li></ul><p><code>nano masters</code></p><p>hdp-node-01</p><p><strong>PS. 其实可以在一台机子上安装和配置好，然后用scp命令直接复制整个文件夹给从机。但阿里云外网IP间传输实在是太慢了！</strong></p><h3 id="7-启动集群"><a href="#7-启动集群" class="headerlink" title="7 启动集群"></a>7 启动集群</h3><ul><li>初始化HDFS</li></ul><p><code>hdfs namenode -format</code></p><ul><li>启动HDFS以及YARN</li></ul><p><code>bash start-all.sh</code></p><p>集群启动完毕。在浏览器上可观察启动状况</p><p>输入<code>主机IP:50070</code>，查看大体配置情况。主要注意存活节点数，我这里是2个slaves。</p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/YO1pjbypw23F1YFsj3JSKcg4a.enRUaPLDcVt6RdU.I!/r/dDQBAAAAAAAA&amp;bo=rwV5Aq8FeQIDGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540315053705"></p><p>输入<code>主机IP:8088</code>，可查看当前运行情况。这里我在跑一个wordcount的demo，可看到进度情况。</p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/nJKXAMQNX*dYe1n4jm9jt9yYVEBRuovkAkXj6FjZNFM!/r/dGcBAAAAAAAA&amp;bo=OAeAAngHlgIDCXo!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540314812172"></p><ul><li>建立input目录</li></ul><p><code>hadoop fs -mkdir -p /wordcount/input</code></p><ul><li>放入文件到该目录下</li></ul><p><code>hadoop fs -put /usr/hello.txt /wordcount/input</code></p><ul><li>跑demo</li></ul><p><code>cd ./share/hadoop/mapreduce/</code></p><p><code>hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount /wordcount/input/hello.txt output2</code></p><p>在控制台中可看到，表示运行成功：</p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/UVHTTWylwbde3iwZgxsUSq7YiFS1bdv.n6*I6Ptpj*c!/r/dDcBAAAAAAAA&amp;bo=sgNtAbIDbQEDGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540315291480"></p><p>我的hello.txt的内容是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hello World Bye World</span><br><span class="line">Hello World Bye World</span><br><span class="line">Hello World Bye World</span><br><span class="line">Hello World Bye World</span><br><span class="line">Hello World Bye World</span><br></pre></td></tr></table></figure><p>取出wordcount统计结果：</p><p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/JkLrC2ZsvrSocWq43DnOHOi08v5BYr46qzCDYUTzlXU!/r/dEgBAAAAAAAA&amp;bo=OANVADgDVQADGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540315844610"></p><p>这就是Hadoop世界的HelloWorld代码，我们环境搭建也随之完成。</p><p>参考：</p><p>阿里云内网IP搭建：<a href="https://blog.csdn.net/bqw18744018044/article/details/79103931" target="_blank" rel="noopener">https://blog.csdn.net/bqw18744018044/article/details/79103931</a></p><p>阿里云内网IP搭建：<a href="https://blog.csdn.net/QianZhaoVic/article/details/83150703" target="_blank" rel="noopener">https://blog.csdn.net/QianZhaoVic/article/details/83150703</a></p><p>hadoop参数：<a href="https://blog.csdn.net/lydia88/article/details/79449656" target="_blank" rel="noopener">https://blog.csdn.net/lydia88/article/details/79449656</a></p><h2 id="二、基于HIVE的数据分析环境搭建"><a href="#二、基于HIVE的数据分析环境搭建" class="headerlink" title="二、基于HIVE的数据分析环境搭建"></a>二、基于HIVE的数据分析环境搭建</h2><p>因为课程作业环境的需求，这一次要基于HADOOP集群正常运行情况下，使用HIVE等工具进行数据分析，因此在hadoop集群搭建完成的基础上，安装Hive+Sqoop+HBASE+mysql。衔接上面的内容。</p><p>先到镜像站下载所需安装包：<a href="http://mirror.bit.edu.cn/apache/" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/</a><br>总结下我目前现在安装的各个环境的版本</p><ul><li>jdk-8u191-linux-x64.tar.gz</li><li>hadoop-2.7.7.tar.gz</li><li>hbase-1.3.2.1-bin.tar.gz</li><li>apache-hive-2.3.3-bin.tar.gz</li><li>sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</li><li>mysql 4.6.42</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>这里以hive为例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-2.3.3-bin.tar.gz -C /usr/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv apache-hive-2.3.3-bin hive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nano /etc/profile</span><br></pre></td></tr></table></figure><p>在下方添加，一口气配齐环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/usr/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">export HBASE_HOME=/usr/hbase</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br><span class="line">export SQOOP_HOME=/usr/sqoop</span><br><span class="line">export PATH=$PATH:$SQOOP_HOME/bin</span><br></pre></td></tr></table></figure><p>生效环境变量配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>到此，hbase、sqoop、hive都是这样安装，并配置系统环境变量。</p><p>然后我们开始部署。HBASE是比较独立的一个模块，我们先部署它。</p><h2 id="HBASE的部署"><a href="#HBASE的部署" class="headerlink" title="HBASE的部署"></a>HBASE的部署</h2><p>修改conf文件夹下的hbase-env.sh，找到下方部分修改java_home路径。并找到HBASE_MANAGES_ZK设为true，这里使用HBASE自带的zookeeper。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line">export JAVA_HOME=/usr/java</span><br><span class="line">...中间省略</span><br><span class="line"># Tell HBase whether it should manage it&apos;s own instance of Zookeeper or not.</span><br><span class="line">export HBASE_MANAGES_ZK=true</span><br></pre></td></tr></table></figure><p>新建一份hbase-site.xml，我的配置如下：注意hbase.zookeeper.quorum的配置，主机和从机有分歧。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">　　　　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- hbase存放数据目录 --&gt;</span></span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hdp-node-01:9000/opt/hbase/hbase_db<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- 端口要和Hadoop的fs.defaultFS端口一致--&gt;</span></span><br><span class="line">　　　　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line">　　　　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- 是否分布式部署 --&gt;</span></span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">　　　　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line">　　　　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- list of  zookooper。--&gt;</span></span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- 这里是slaves配置的情况。had-node-01作为Hmaster节点，此处写hdp-node-01。--&gt;</span></span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdp-node-02,hdp-node-03<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">　　　　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span> 　　　 </span><br><span class="line"></span><br><span class="line">　　　　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">　　　　　　 <span class="comment">&lt;!--zookooper配置、日志等的存储位置 --&gt;</span></span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hbase/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　　　　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置regionservers：</p><p>编辑HBASE目录下conf/regionservers   去掉默认的localhost，加入hdp-node-02、hdp-node-03，保存退出</p><p>配置完成后，启动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><p>输入jps命令查看进程是否启动成功</p><p> hdp-node-01上出现HMaster、HQuormPeer，slaves上出现HRegionServer、HQuorumPeer，就是启动成功了。</p><p>运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase shell</span><br></pre></td></tr></table></figure><p>输入status命令可以看到如下内容，1个master，2 servers，则3机器全部成功启动。</p><p>并在<a href="http://hdp-node-01:16010/" target="_blank" rel="noopener">http://hdp-node-01:16010/</a> 可以看到详细信息。</p><p>接下去部署HIVE，是使用类SQL语言分析数据的必备工具。</p><h2 id="HIVE部署"><a href="#HIVE部署" class="headerlink" title="HIVE部署"></a>HIVE部署</h2><p>修改hive-env.sh文件，添加路径</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set HADOOP_HOME to point to a specific hadoop install directory</span></span><br><span class="line"> HADOOP_HOME=/usr/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive Configuration Directory can be controlled by:</span></span><br><span class="line"> <span class="built_in">export</span> HIVE_CONF_DIR=/usr/hive/conf</span><br></pre></td></tr></table></figure><p>复制hive-default.xml.template，并重命名为hive-site.xml。Hive 系统会加载这两个配置文件。当“hive-site.xml”中的配置参数的值与“hive-default.xml”文件中不一致时，以用户自定义的为准。故hive-site.xml填写如下，其余部分可删除：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">1   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Hive Metastore有三种配置方式，Derby、Local和Remote。</p><p>内嵌模式（Derby）使用的是内嵌的Derby数据库来存储元数据，也不需要额外起Metastore服务。这个是默认的，配置简单，但是一次只能一个客户端连接，适用于用来实验，不适用于生产环境。</p><p>这里用了Local本地元存储的方式。在生产环境中，建议用远程元存储来配置Hive Metastore。</p><p>本地元存储和远程元存储都采用外部数据库来存储元数据，目前支持的数据库有：MySQL、Postgres、Oracle、MS SQL Server。在这里我们使用MySQL。</p><p>接下来配置连接地址以及驱动，最后两个property是mysql的用户和密码。</p><p>因为使用MySQL作为存储元数据的数据库，所以需要把连接MySQL的jar包（mysql-connector-java-XXX.jar）放入到$HIVE_HOME/lib目录下。我直接用ftp传上去了。</p><p>既然配置到了mysql，下面我们就来安装配置mysql。</p><h2 id="mysql安装部署"><a href="#mysql安装部署" class="headerlink" title="mysql安装部署"></a>mysql安装部署</h2><p>确保之前未安装过mysql，若有一定要卸载干净再开始安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install mysql-server</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysqld start</span><br></pre></td></tr></table></figure><p>登录mysql，为Hive建立相应的MySQL账户，并赋予足够的权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE USER &apos;hive&apos; IDENTIFIED BY &apos;mysql&apos;;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;hive&apos;@&apos;%&apos; WITH GRANT OPTION;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure><p>重启服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql restart</span><br></pre></td></tr></table></figure><p>建立hive专用元数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; exit;</span><br><span class="line">root@hdp-node-01:~$ mysql -uhive -pmysql</span><br><span class="line">mysql&gt; create database hive;</span><br></pre></td></tr></table></figure><p>配置完成后，运行hive</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure><p>若不报错，则部署成功。</p><p>若报错为：</p><p><img src="http://r.photo.store.qq.com/psb?/V10ldIv84XBZXi/9st2MTzlQC4U09TP0LgQn8JVolbFWH0RKTOqmDg1NNM!/r/dDABAAAAAAAA&amp;bo=LgNBAC4DQQADCSw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1"></p><p>在确保mysql正确安装及运行的情况下，应该是Hive2需要hive元数据库初始化，运行：<br><code>schematool -dbType mysql -initSchema</code></p><p>若报错为2002，则为mysql服务未启动</p><p><img src="http://r.photo.store.qq.com/psb?/V10ldIv84XBZXi/n*0lmwRfjJLcGuR89q0Rj.S1K9b12bYEu0VxE2ozglI!/r/dDYBAAAAAAAA&amp;bo=LwNBAC8DQQADGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="2"></p><p>测试一下hive的运行情况，已经可以使用类SQL语言进行操作：</p><p><img src="http://r.photo.store.qq.com/psb?/V10ldIv84XBZXi/eSz.iOrtlapqyUD.48KvIbW.QAtmnNbHXYu1f8yZMjs!/r/dDUBAAAAAAAA&amp;bo=PQN9Az0DfQMDCSw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540953069204"></p><p>最后，我们配置下sqoop，以后可能会用到。</p><h2 id="Sqoop配置"><a href="#Sqoop配置" class="headerlink" title="Sqoop配置"></a>Sqoop配置</h2><p>修改sqoop-env.sh</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/usr/hadoop</span><br><span class="line"></span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/usr/hadoop</span><br><span class="line"></span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">export HBASE_HOME=/usr/hbase</span><br><span class="line"></span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/usr/hive</span><br></pre></td></tr></table></figure><p>参考：</p><p>Hadoop+HBASE：<a href="https://www.cnblogs.com/lzxlfly/p/7221890.html" target="_blank" rel="noopener">https://www.cnblogs.com/lzxlfly/p/7221890.html</a></p><p>Hive Metastore解释：<a href="https://www.cnblogs.com/linbingdong/p/5829369.html" target="_blank" rel="noopener">https://www.cnblogs.com/linbingdong/p/5829369.html</a></p><p>mysql+Hive部署：<a href="https://www.cnblogs.com/kxdblog/p/4100263.html" target="_blank" rel="noopener">https://www.cnblogs.com/kxdblog/p/4100263.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着云服务器的飞速发展，伴随着&lt;strong&gt;云平台&lt;/strong&gt;的各种巨大优势所在，在云上&lt;strong&gt;配置Hadoop集群&lt;/strong&gt;似乎是最正确的选择。&lt;/p&gt;
&lt;p&gt;这次用了3台&lt;strong&gt;阿里云ECS服务器&lt;/strong&gt;（学生机）来完成这次的环境搭建。由于是三个不同的账号（每个账号的学生优惠只能有一个实例），所以走的是&lt;strong&gt;外网IP通道&lt;/strong&gt;，和网上大部分教程有一定的区别。&lt;/p&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://yhcheer.com/categories/BigData/"/>
    
    
      <category term="Hadoop" scheme="http://yhcheer.com/tags/Hadoop/"/>
    
      <category term="大数据" scheme="http://yhcheer.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hive" scheme="http://yhcheer.com/tags/Hive/"/>
    
      <category term="Spark" scheme="http://yhcheer.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>实现一个光线追踪渲染器</title>
    <link href="http://yhcheer.com/2018/10/20/ray-tarcing-in-one-weekend/"/>
    <id>http://yhcheer.com/2018/10/20/ray-tarcing-in-one-weekend/</id>
    <published>2018-10-20T14:48:33.000Z</published>
    <updated>2019-03-12T15:07:34.081Z</updated>
    
    <content type="html"><![CDATA[<p>细读<strong>《Ray tracing in one weekend》</strong>，<strong>PeterShirly</strong>用C++语言加上一些简单的物理知识，带我们实现一个简单的光线追踪渲染器。这里是我用<strong>Java</strong>复现后的结果展示。每一章节工程代码已上传至<a href="https://github.com/yhcheer/RayTracingInOneWeekend" target="_blank" rel="noopener">Github</a>，具体实现的步骤和一些感悟写在了<a href="https://zhuanlan.zhihu.com/p/49943215" target="_blank" rel="noopener">知乎专栏</a>，如果有意见和建议还请指出，感谢！</p><a id="more"></a><p>上半部分：<a href="https://zhuanlan.zhihu.com/p/49943215" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49943215</a></p><p>下半部分：<a href="https://zhuanlan.zhihu.com/p/50451925" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/50451925</a></p><h3 id="Chapter-12-Where-next"><a href="#Chapter-12-Where-next" class="headerlink" title="Chapter 12:  Where next?"></a>Chapter 12:  Where next?</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp0.jpg" alt=""></p><h3 id="Chapter-11-Defocus-Blur"><a href="#Chapter-11-Defocus-Blur" class="headerlink" title="Chapter 11:   Defocus Blur"></a>Chapter 11:   Defocus Blur</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp11.jpg" alt="cp11"></p><h3 id="Chapter-10-Positionable-camera"><a href="#Chapter-10-Positionable-camera" class="headerlink" title="Chapter 10:  Positionable camera"></a>Chapter 10:  Positionable camera</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp10.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp10_2.jpg" alt=""></p><h3 id="Chapter-9-Dielectrics"><a href="#Chapter-9-Dielectrics" class="headerlink" title="Chapter 9:  Dielectrics"></a>Chapter 9:  Dielectrics</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp9.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp9_2.jpg" alt=""></p><h3 id="Chapter-8-Metal"><a href="#Chapter-8-Metal" class="headerlink" title="Chapter 8:  Metal"></a>Chapter 8:  Metal</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp8.jpg" alt="cp8"></p><h3 id="Chapter-7-Diffuse-Materials"><a href="#Chapter-7-Diffuse-Materials" class="headerlink" title="Chapter 7:    Diffuse Materials"></a>Chapter 7:    Diffuse Materials</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp7.jpg" alt=""></p><h3 id="Chapter-6-Antialiasing"><a href="#Chapter-6-Antialiasing" class="headerlink" title="Chapter 6:  Antialiasing"></a>Chapter 6:  Antialiasing</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp6.jpg" alt="Cp6"></p><h3 id="Chapter-5-Surface-normals-and-multiple-objects"><a href="#Chapter-5-Surface-normals-and-multiple-objects" class="headerlink" title="Chapter 5:   Surface normals and  multiple objects."></a>Chapter 5:   Surface normals and  multiple objects.</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp5_1.jpg" alt="cp5-1"></p><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp5_3.jpg" alt=""></p><h3 id="Chapter-4-Adding-a-sphere"><a href="#Chapter-4-Adding-a-sphere" class="headerlink" title="Chapter 4:  Adding a sphere"></a>Chapter 4:  Adding a sphere</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp4.jpg" alt="CP4"></p><h3 id="Chapter-3-Rays-a-simple-camera-and-background"><a href="#Chapter-3-Rays-a-simple-camera-and-background" class="headerlink" title="Chapter 3:  Rays, a simple camera, and background"></a>Chapter 3:  Rays, a simple camera, and background</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp3.jpg" alt="Cp3"></p><h3 id="Chapter-2-The-vec3-class"><a href="#Chapter-2-The-vec3-class" class="headerlink" title="Chapter 2:   The vec3 class"></a>Chapter 2:   The vec3 class</h3><h3 id="Chapter-1：Output-an-image"><a href="#Chapter-1：Output-an-image" class="headerlink" title="Chapter 1：Output an image"></a>Chapter 1：Output an image</h3><p><img src="https://raw.githubusercontent.com/yhcheer/RayTracingInOneWeekend/master/image/Cp1.jpg" alt="cp1"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;细读&lt;strong&gt;《Ray tracing in one weekend》&lt;/strong&gt;，&lt;strong&gt;PeterShirly&lt;/strong&gt;用C++语言加上一些简单的物理知识，带我们实现一个简单的光线追踪渲染器。这里是我用&lt;strong&gt;Java&lt;/strong&gt;复现后的结果展示。每一章节工程代码已上传至&lt;a href=&quot;https://github.com/yhcheer/RayTracingInOneWeekend&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github&lt;/a&gt;，具体实现的步骤和一些感悟写在了&lt;a href=&quot;https://zhuanlan.zhihu.com/p/49943215&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;知乎专栏&lt;/a&gt;，如果有意见和建议还请指出，感谢！&lt;/p&gt;
    
    </summary>
    
      <category term="CG" scheme="http://yhcheer.com/categories/CG/"/>
    
    
      <category term="计算机图形学" scheme="http://yhcheer.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://yhcheer.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Java" scheme="http://yhcheer.com/tags/Java/"/>
    
      <category term="光线追踪" scheme="http://yhcheer.com/tags/%E5%85%89%E7%BA%BF%E8%BF%BD%E8%B8%AA/"/>
    
  </entry>
  
</feed>
