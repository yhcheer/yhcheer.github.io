<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="yh">
  <!-- Open Graph Data -->
  <meta property="og:title" content="用阿里云搭建Hadoop集群">
  <meta property="og:description" content="">
  <meta property="og:site_name" content="樱花飘落之时">
  <meta property="og:type" content="article">
  <meta property="og:image" content="http://yhcheer.com">
  
    <link rel="alternate" href="/atom.xml" title="樱花飘落之时" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>樱花飘落之时</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/yinghua.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">用阿里云搭建Hadoop集群</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/yhcheer">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:yhcheer@zju.edu.cn">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By yh</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2018-10-23</span>
            <span class="time">23:23:52</span>
          </span>
          
          <!--  Categories  -->
            <span class="categories info">Under 

<a href="/categories/大数据/">大数据</a>
</span>
          
        </div>
        <!-- Tags -->
        
          <div class="post-tags text-muted">
            Tags: 

<a class="tag" href="/tags/Hadoop/">#Hadoop</a> <a class="tag" href="/tags/大数据/">#大数据</a> <a class="tag" href="/tags/Hive/">#Hive</a> <a class="tag" href="/tags/Spark/">#Spark</a>


          </div>
        
        <!-- Post Main Content -->
        <div class="post-content">
          <p>随着云服务器的飞速发展，伴随着<strong>云平台</strong>的各种巨大优势所在，在云上<strong>配置Hadoop集群</strong>似乎是最正确的选择。</p>
<p>这次用了3台<strong>阿里云ECS服务器</strong>（学生机）来完成这次的环境搭建。由于是三个不同的账号（每个账号的学生优惠只能有一个实例），所以走的是<strong>外网IP通道</strong>，和网上大部分教程有一定的区别。</p>
<a id="more"></a>
<h2 id="一、HADOOP集群搭建"><a href="#一、HADOOP集群搭建" class="headerlink" title="一、HADOOP集群搭建"></a>一、HADOOP集群搭建</h2><h3 id="1-服务器准备"><a href="#1-服务器准备" class="headerlink" title="1 服务器准备"></a>1 服务器准备</h3><p>阿里云ESC服务器 1 vCPU 2 GB (I/O优化) ecs.n4.small 1Mbps </p>
<p>默认系统选的Centos 7.4 64bit</p>
<p>阿里云已经导入学信网系统，学生优惠9.5/月，你值得拥有</p>
<h3 id="2-网络环境准备"><a href="#2-网络环境准备" class="headerlink" title="2 网络环境准备"></a>2 网络环境准备</h3><p>阿里云自动分配公网IP和私网IP，记住即可</p>
<p>阿里云的安全组设置默认只开启22端口用于SSH，以及3389端口用于远程登录</p>
<p>对于搭hadoop集群的我们肯定是不够的，如果你不太了解hadoop的端口配置，就到阿里云控制台上在你的实例的安全组上，添加如下一步到位的规则：</p>
<p>规则方向-出方向和入方向，协议类型-全部，授权类型-地址段访问，授权对象-0.0.0.0/0</p>
<p>当然这样是很不安全的，但对于新手来说，省了很多麻烦，建议后期手动修改安全组设置。</p>
<h3 id="3-服务器系统设置"><a href="#3-服务器系统设置" class="headerlink" title="3 服务器系统设置"></a>3 服务器系统设置</h3><p>3.1 添加HADOOP用户</p>
<p><del>不添加。</del>我是用root用户来搭建。</p>
<p><strong>PS. 我第一次搭的时候，使用的是HADOOP用户，但尝试了各种方法，都无法成功在HADOOP用户下进行ssh免密，导致最后跑集群的时候出错。</strong></p>
<p>3.2 分配root权限</p>
<p><del>root用户无需再分配</del></p>
<p>3.3 同步时间</p>
<p><del>阿里云无需同步时间</del></p>
<p>3.4 设置主机名（重要）</p>
<p><code>sudo nano /etc/hostname</code></p>
<p>我习惯用nano，没有的话装一下<code>yum install nano</code></p>
<p>如果是自己的主机的话直接在“阿里云实例设置-修改信息-重启实例”也能改。</p>
<p>三台机器主机名分别设y为：</p>
<ul>
<li>hdp-node-01</li>
<li>hdp-node-02</li>
<li>hdp-node-03</li>
</ul>
<p>3.5 域名映射（重要）</p>
<p>将一些常用的网址域名与其对应的IP地址建立一个关联“数据库”</p>
<p><code>sudo nano /etc/hosts</code></p>
<p>在<strong>文件最下方</strong>添加如下信息，我这里用外网IP。如果是在同一个账号下的三个实例，可以选择走内网IP通道，速度上应该会快很多。</p>
<ul>
<li>本机<strong>内网IP</strong> hdp-node-01</li>
<li>从机外网IP hdp-node-02</li>
<li>从机外网IP hdp-node-03</li>
</ul>
<p><strong>PS. 这里一开始我用的都是外网IP，最后报错，原因大概是阿里云的外网IP冲突问题，因此改为内网IP。注意只有主机的hosts配置的第一个节点（本机）是内网IP，并且另外几台从机的hosts配置都是使用外网IP不用改变。</strong></p>
<p>3.6 配置ssh免密登录（重要）</p>
<p><code>ssh-keygen</code>利用ssh-keygen生成密钥</p>
<p><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code>加入到授权，可以免密ssh自己</p>
<p>把密匙公钥发送给别的节点，已达到免密的效果，发送时要输入对面节点的密码</p>
<p><code>scp ~/.ssh/id_rsa.pub root@hdp-node-02:~/.ssh</code>将id_rsa.pub拷贝到hdp-node-02节点上</p>
<p><code>scp ~/.ssh/id_rsa.pub root@hdp-node-03:~/.ssh</code>将id_rsa.pub拷贝到hdp-node-03节点上</p>
<p><strong>换到hdp-node-02和hdp-node-03上操作：</strong></p>
<p><code>cat ~/.ssh/id_rsa.pub &gt;&gt;~/.ssh/authorized_keys</code>把刚才的密匙加入到授权</p>
<p><strong>回到hdp-node-01上操作：</strong></p>
<p>测试<code>ssh hdp-node-02</code>成功！如果还需要输入密码则免密失败。</p>
<p><code>logout</code>退出刚才的ssh</p>
<p><strong>PS. 后来调试bug的时候，我把所有节点相互之间的ssh免密也实现了，理论上应该只需要主机ssh所有从机即可。</strong></p>
<p>3.7 配置防火墙</p>
<p><code>systemctl stop firewalld</code>关闭centos7防火墙</p>
<h3 id="4-JDK环境安装"><a href="#4-JDK环境安装" class="headerlink" title="4 JDK环境安装"></a>4 JDK环境安装</h3><p>JDK是HADOOP的必要环境</p>
<p>下载jdk1.8，可以用wget方法，我就用FileZilla传上去再解压好了</p>
<p>放到/usr文件夹下，解压</p>
<p><code>tar -zxf jdk-8u191-linux-x64.tar.gz</code></p>
<p>重命名一下文件夹名字为java（非必须）</p>
<p><code>mv jdk1.8.0_191 java</code></p>
<p>配置环境变量<code>nano /etc/profile</code>，在<strong>文件最下方</strong>添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>生效环境<code>source /etc/profile</code></p>
<p>测试<code>java -version</code>，成功则显示版本号</p>
<p>重复以上步骤在每个从机上执行</p>
<h3 id="5-hadoop安装和部署"><a href="#5-hadoop安装和部署" class="headerlink" title="5 hadoop安装和部署"></a>5 hadoop安装和部署</h3><p>下载、解压。同上。</p>
<p>重命名一下（非必须）</p>
<p><code>mv hadoop2.7.7 hadoop</code></p>
<p><strong>PS. 我这里使用的是hadoop 2.7.7版本，注意hadoop 3与hadoop 2的改动比较大，包括端口、命令、以及一些配置等，注意区别。</strong></p>
<p>配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java</span><br><span class="line">export PATH=PATH:$JAVA_HOME/bin</span><br><span class="line">export HADOOP_HOME=/usr/hadoop</span><br><span class="line">export PATH=PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>
<p>生效环境<code>source /etc/profile</code></p>
<p>测试<code>hadoop version</code>显示版本号，则成功</p>
<h3 id="6-开始部署hadoop"><a href="#6-开始部署hadoop" class="headerlink" title="6 开始部署hadoop"></a>6 开始部署hadoop</h3><p>进入hadoop安装目录下的<strong>子目录</strong>/etc/hadoop</p>
<p><code>cd /usr/hadoop/etc/hadoop</code></p>
<ul>
<li>配置JAVA路径</li>
</ul>
<p><code>nano hadoop-env.sh</code>找到下面写有JAVA_HOME的地方，修改JAVA路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.</span><br><span class="line">export JAVA_HOME=/usr/java</span><br></pre></td></tr></table></figure>
<ul>
<li>配置core-site.xml</li>
</ul>
<p>集群全局参数，用于定义系统级别的参数，如HDFS  URL、Hadoop的临时目录等</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hdp-node-01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数名</th>
<th>默认值</th>
<th>参数解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>fs.defaultFS</td>
<td>file:///</td>
<td>文件系统主机和端口</td>
</tr>
<tr>
<td>2</td>
<td>io.file.buffer.size</td>
<td>4096</td>
<td>流文件的缓冲区大小</td>
</tr>
<tr>
<td>3</td>
<td>hadoop.tmp.dir</td>
<td>/tmp/hadoop-${user.name}</td>
<td>临时文件夹</td>
</tr>
</tbody>
</table>
<ul>
<li>配置hdfs-site.xml</li>
</ul>
<p>HDFS参数，如名称节点和数据节点的存放位置、文件副本的个数、文件读取权限等</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdp-node-02:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.https-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdp-node-02:50091<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数名</th>
<th>默认值</th>
<th>参数解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>dfs.namenode.secondary.http-address</td>
<td>0.0.0.0:50090</td>
<td>定义HDFS对应的HTTP服务器地址和端口</td>
</tr>
<tr>
<td>2</td>
<td>dfs.namenode.name.dir</td>
<td>file://${hadoop.tmp.dir}/dfs/name</td>
<td>定义DFS的名称节点在本地文件系统的位置</td>
</tr>
<tr>
<td>3</td>
<td>dfs.datanode.data.dir</td>
<td>file://${hadoop.tmp.dir}/dfs/data</td>
<td>定义DFS数据节点存储数据块时存储在本地文件系统的位置</td>
</tr>
<tr>
<td>4</td>
<td>dfs.replication</td>
<td>3</td>
<td>缺省的块复制数量</td>
</tr>
<tr>
<td>5</td>
<td>dfs.webhdfs.enabled</td>
<td>true</td>
<td>是否通过http协议读取hdfs文件，如果选是，则集群安全性较差</td>
</tr>
</tbody>
</table>
<ul>
<li>配置mapred-site.xml</li>
</ul>
<p>Mapreduce参数，包括JobHistory Server和应用程序参数两部分，如reduce任务的默认个数、任务所能够使用内存的默认上下限等</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数名</th>
<th>默认值</th>
<th>参数解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>mapreduce.framework.name</td>
<td>local</td>
<td>取值local、classic或yarn其中之一，如果不是yarn，则不会使用YARN集群来实现资源的分配</td>
</tr>
<tr>
<td>2</td>
<td>mapreduce.jobhistory.address</td>
<td>0.0.0.0:10020</td>
<td>定义历史服务器的地址和端口，通过历史服务器查看已经运行完的Mapreduce作业记录</td>
</tr>
<tr>
<td>3</td>
<td>mapreduce.jobhistory.webapp.address</td>
<td>0.0.0.0:19888</td>
<td>定义历史服务器web应用访问的地址和端口</td>
</tr>
</tbody>
</table>
<ul>
<li>配置yarn-site.xml</li>
</ul>
<p>集群资源管理系统参数，配置 ResourceManager，NodeManager 的通信端口，web监控端口等</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- RM的hostname --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!--指定Yarn的老大(ResourceManager)的地址--&gt;</span>  </span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdp-node-01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">	<span class="comment">&lt;!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数名</th>
<th>默认值</th>
<th>参数解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>yarn.resourcemanager.address</td>
<td>0.0.0.0:8032</td>
<td>ResourceManager 提供给客户端访问的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等</td>
</tr>
<tr>
<td>2</td>
<td>yarn.resourcemanager.scheduler.address</td>
<td>0.0.0.0:8030</td>
<td>ResourceManager提供给ApplicationMaster的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等</td>
</tr>
<tr>
<td>3</td>
<td>yarn.resourcemanager.resource-tracker.address</td>
<td>0.0.0.0:8031</td>
<td>ResourceManager 提供给NodeManager的地址。NodeManager通过该地址向RM汇报心跳，领取任务等</td>
</tr>
<tr>
<td>4</td>
<td>yarn.resourcemanager.admin.address</td>
<td>0.0.0.0:8033</td>
<td>ResourceManager 提供给管理员的访问地址。管理员通过该地址向RM发送管理命令等。</td>
</tr>
<tr>
<td>5</td>
<td>yarn.resourcemanager.webapp.address</td>
<td>0.0.0.0:8088</td>
<td>ResourceManager对web 服务提供地址。用户可通过该地址在浏览器中查看集群各类信息</td>
</tr>
<tr>
<td>6</td>
<td>yarn.nodemanager.aux-services</td>
<td></td>
<td>通过该配置项，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的，这样就可以在NodeManager上扩展自己的服务。</td>
</tr>
</tbody>
</table>
<ul>
<li>配置slaves</li>
</ul>
<p><code>nano slaves</code></p>
<p>hdp-node-02<br>hdp-node-03</p>
<ul>
<li>配置masters</li>
</ul>
<p><code>nano masters</code></p>
<p>hdp-node-01</p>
<p><strong>PS. 其实可以在一台机子上安装和配置好，然后用scp命令直接复制整个文件夹给从机。但阿里云外网IP间传输实在是太慢了！</strong></p>
<h3 id="7-启动集群"><a href="#7-启动集群" class="headerlink" title="7 启动集群"></a>7 启动集群</h3><ul>
<li>初始化HDFS</li>
</ul>
<p><code>hdfs namenode -format</code></p>
<ul>
<li>启动HDFS以及YARN</li>
</ul>
<p><code>bash start-all.sh</code></p>
<p>集群启动完毕。在浏览器上可观察启动状况</p>
<p>输入<code>主机IP:50070</code>，查看大体配置情况。主要注意存活节点数，我这里是2个slaves。</p>
<p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/YO1pjbypw23F1YFsj3JSKcg4a.enRUaPLDcVt6RdU.I!/r/dDQBAAAAAAAA&amp;bo=rwV5Aq8FeQIDGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540315053705"></p>
<p>输入<code>主机IP:8088</code>，可查看当前运行情况。这里我在跑一个wordcount的demo，可看到进度情况。</p>
<p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/nJKXAMQNX*dYe1n4jm9jt9yYVEBRuovkAkXj6FjZNFM!/r/dGcBAAAAAAAA&amp;bo=OAeAAngHlgIDCXo!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540314812172"></p>
<ul>
<li>建立input目录</li>
</ul>
<p><code>hadoop fs -mkdir -p /wordcount/input</code></p>
<ul>
<li>放入文件到该目录下</li>
</ul>
<p><code>hadoop fs -put /usr/hello.txt /wordcount/input</code></p>
<ul>
<li>跑demo</li>
</ul>
<p><code>cd ./share/hadoop/mapreduce/</code></p>
<p><code>hadoop jar hadoop-mapreduce-examples-2.7.7.jar wordcount /wordcount/input/hello.txt output2</code></p>
<p>在控制台中可看到，表示运行成功：</p>
<p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/UVHTTWylwbde3iwZgxsUSq7YiFS1bdv.n6*I6Ptpj*c!/r/dDcBAAAAAAAA&amp;bo=sgNtAbIDbQEDGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540315291480"></p>
<p>我的hello.txt的内容是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hello World Bye World</span><br><span class="line">Hello World Bye World</span><br><span class="line">Hello World Bye World</span><br><span class="line">Hello World Bye World</span><br><span class="line">Hello World Bye World</span><br></pre></td></tr></table></figure>
<p>取出wordcount统计结果：</p>
<p><img src="//r.photo.store.qq.com/psb?/V10ldIv84XBZXi/JkLrC2ZsvrSocWq43DnOHOi08v5BYr46qzCDYUTzlXU!/r/dEgBAAAAAAAA&amp;bo=OANVADgDVQADGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540315844610"></p>
<p>这就是Hadoop世界的HelloWorld代码，我们环境搭建也随之完成。</p>
<p>参考：</p>
<p>阿里云内网IP搭建：<a href="https://blog.csdn.net/bqw18744018044/article/details/79103931" target="_blank" rel="noopener">https://blog.csdn.net/bqw18744018044/article/details/79103931</a></p>
<p>阿里云内网IP搭建：<a href="https://blog.csdn.net/QianZhaoVic/article/details/83150703" target="_blank" rel="noopener">https://blog.csdn.net/QianZhaoVic/article/details/83150703</a></p>
<p>hadoop参数：<a href="https://blog.csdn.net/lydia88/article/details/79449656" target="_blank" rel="noopener">https://blog.csdn.net/lydia88/article/details/79449656</a></p>
<h2 id="二、基于HIVE的数据分析环境搭建"><a href="#二、基于HIVE的数据分析环境搭建" class="headerlink" title="二、基于HIVE的数据分析环境搭建"></a>二、基于HIVE的数据分析环境搭建</h2><p>因为课程作业环境的需求，这一次要基于HADOOP集群正常运行情况下，使用HIVE等工具进行数据分析，因此在hadoop集群搭建完成的基础上，安装Hive+Sqoop+HBASE+mysql。衔接上面的内容。</p>
<p>先到镜像站下载所需安装包：<a href="http://mirror.bit.edu.cn/apache/" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/</a><br>总结下我目前现在安装的各个环境的版本</p>
<ul>
<li>jdk-8u191-linux-x64.tar.gz</li>
<li>hadoop-2.7.7.tar.gz</li>
<li>hbase-1.3.2.1-bin.tar.gz</li>
<li>apache-hive-2.3.3-bin.tar.gz</li>
<li>sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</li>
<li>mysql 4.6.42</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>这里以hive为例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-2.3.3-bin.tar.gz -C /usr/</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv apache-hive-2.3.3-bin hive</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nano /etc/profile</span><br></pre></td></tr></table></figure>
<p>在下方添加，一口气配齐环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/usr/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">export HBASE_HOME=/usr/hbase</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br><span class="line">export SQOOP_HOME=/usr/sqoop</span><br><span class="line">export PATH=$PATH:$SQOOP_HOME/bin</span><br></pre></td></tr></table></figure>
<p>生效环境变量配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<p>到此，hbase、sqoop、hive都是这样安装，并配置系统环境变量。</p>
<p>然后我们开始部署。HBASE是比较独立的一个模块，我们先部署它。</p>
<h2 id="HBASE的部署"><a href="#HBASE的部署" class="headerlink" title="HBASE的部署"></a>HBASE的部署</h2><p>修改conf文件夹下的hbase-env.sh，找到下方部分修改java_home路径。并找到HBASE_MANAGES_ZK设为true，这里使用HBASE自带的zookeeper。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line">export JAVA_HOME=/usr/java</span><br><span class="line">...中间省略</span><br><span class="line"># Tell HBase whether it should manage it&apos;s own instance of Zookeeper or not.</span><br><span class="line">export HBASE_MANAGES_ZK=true</span><br></pre></td></tr></table></figure>
<p>新建一份hbase-site.xml，我的配置如下：注意hbase.zookeeper.quorum的配置，主机和从机有分歧。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">　　　　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- hbase存放数据目录 --&gt;</span></span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hdp-node-01:9000/opt/hbase/hbase_db<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- 端口要和Hadoop的fs.defaultFS端口一致--&gt;</span></span><br><span class="line">　　　　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line">　　　　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- 是否分布式部署 --&gt;</span></span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">　　　　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line">　　　　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- list of  zookooper。--&gt;</span></span><br><span class="line">　　　　　　　　<span class="comment">&lt;!-- 这里是slaves配置的情况。had-node-01作为Hmaster节点，此处写hdp-node-01。--&gt;</span></span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdp-node-02,hdp-node-03<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">　　　　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span> 　　　 </span><br><span class="line"></span><br><span class="line">　　　　　　<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">　　　　　　	 <span class="comment">&lt;!--zookooper配置、日志等的存储位置 --&gt;</span></span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">　　　　　　　　<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hbase/zookeeper<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">　　　　　　<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>配置regionservers：</p>
<p>编辑HBASE目录下conf/regionservers   去掉默认的localhost，加入hdp-node-02、hdp-node-03，保存退出</p>
<p>配置完成后，启动：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure>
<p>输入jps命令查看进程是否启动成功</p>
<p> hdp-node-01上出现HMaster、HQuormPeer，slaves上出现HRegionServer、HQuorumPeer，就是启动成功了。</p>
<p>运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase shell</span><br></pre></td></tr></table></figure>
<p>输入status命令可以看到如下内容，1个master，2 servers，则3机器全部成功启动。</p>
<p>并在<a href="http://hdp-node-01:16010/" target="_blank" rel="noopener">http://hdp-node-01:16010/</a> 可以看到详细信息。</p>
<p>接下去部署HIVE，是使用类SQL语言分析数据的必备工具。</p>
<h2 id="HIVE部署"><a href="#HIVE部署" class="headerlink" title="HIVE部署"></a>HIVE部署</h2><p>修改hive-env.sh文件，添加路径</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set HADOOP_HOME to point to a specific hadoop install directory</span></span><br><span class="line"> HADOOP_HOME=/usr/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive Configuration Directory can be controlled by:</span></span><br><span class="line"> <span class="built_in">export</span> HIVE_CONF_DIR=/usr/hive/conf</span><br></pre></td></tr></table></figure>
<p>复制hive-default.xml.template，并重命名为hive-site.xml。Hive 系统会加载这两个配置文件。当“hive-site.xml”中的配置参数的值与“hive-default.xml”文件中不一致时，以用户自定义的为准。故hive-site.xml填写如下，其余部分可删除：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">1   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Hive Metastore有三种配置方式，Derby、Local和Remote。</p>
<p>内嵌模式（Derby）使用的是内嵌的Derby数据库来存储元数据，也不需要额外起Metastore服务。这个是默认的，配置简单，但是一次只能一个客户端连接，适用于用来实验，不适用于生产环境。</p>
<p>这里用了Local本地元存储的方式。在生产环境中，建议用远程元存储来配置Hive Metastore。</p>
<p>本地元存储和远程元存储都采用外部数据库来存储元数据，目前支持的数据库有：MySQL、Postgres、Oracle、MS SQL Server。在这里我们使用MySQL。</p>
<p>接下来配置连接地址以及驱动，最后两个property是mysql的用户和密码。</p>
<p>因为使用MySQL作为存储元数据的数据库，所以需要把连接MySQL的jar包（mysql-connector-java-XXX.jar）放入到$HIVE_HOME/lib目录下。我直接用ftp传上去了。</p>
<p>既然配置到了mysql，下面我们就来安装配置mysql。</p>
<h2 id="mysql安装部署"><a href="#mysql安装部署" class="headerlink" title="mysql安装部署"></a>mysql安装部署</h2><p>确保之前未安装过mysql，若有一定要卸载干净再开始安装。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install mysql-server</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysqld start</span><br></pre></td></tr></table></figure>
<p>登录mysql，为Hive建立相应的MySQL账户，并赋予足够的权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE USER &apos;hive&apos; IDENTIFIED BY &apos;mysql&apos;;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &apos;hive&apos;@&apos;%&apos; WITH GRANT OPTION;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<p>重启服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql restart</span><br></pre></td></tr></table></figure>
<p>建立hive专用元数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; exit;</span><br><span class="line">root@hdp-node-01:~$ mysql -uhive -pmysql</span><br><span class="line">mysql&gt; create database hive;</span><br></pre></td></tr></table></figure>
<p>配置完成后，运行hive</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>
<p>若不报错，则部署成功。</p>
<p>若报错为：</p>
<p><img src="http://r.photo.store.qq.com/psb?/V10ldIv84XBZXi/9st2MTzlQC4U09TP0LgQn8JVolbFWH0RKTOqmDg1NNM!/r/dDABAAAAAAAA&amp;bo=LgNBAC4DQQADCSw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1"></p>
<p>在确保mysql正确安装及运行的情况下，应该是Hive2需要hive元数据库初始化，运行：<br><code>schematool -dbType mysql -initSchema</code></p>
<p>若报错为2002，则为mysql服务未启动</p>
<p><img src="http://r.photo.store.qq.com/psb?/V10ldIv84XBZXi/n*0lmwRfjJLcGuR89q0Rj.S1K9b12bYEu0VxE2ozglI!/r/dDYBAAAAAAAA&amp;bo=LwNBAC8DQQADGTw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="2"></p>
<p>测试一下hive的运行情况，已经可以使用类SQL语言进行操作：</p>
<p><img src="http://r.photo.store.qq.com/psb?/V10ldIv84XBZXi/eSz.iOrtlapqyUD.48KvIbW.QAtmnNbHXYu1f8yZMjs!/r/dDUBAAAAAAAA&amp;bo=PQN9Az0DfQMDCSw!&amp;rf=viewer_4_yake_qzoneimgout.png" alt="1540953069204"></p>
<p>最后，我们配置下sqoop，以后可能会用到。</p>
<h2 id="Sqoop配置"><a href="#Sqoop配置" class="headerlink" title="Sqoop配置"></a>Sqoop配置</h2><p>修改sqoop-env.sh</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#Set path to where bin/hadoop is available</span><br><span class="line">export HADOOP_COMMON_HOME=/usr/hadoop</span><br><span class="line"></span><br><span class="line">#Set path to where hadoop-*-core.jar is available</span><br><span class="line">export HADOOP_MAPRED_HOME=/usr/hadoop</span><br><span class="line"></span><br><span class="line">#set the path to where bin/hbase is available</span><br><span class="line">export HBASE_HOME=/usr/hbase</span><br><span class="line"></span><br><span class="line">#Set the path to where bin/hive is available</span><br><span class="line">export HIVE_HOME=/usr/hive</span><br></pre></td></tr></table></figure>
<p>参考：</p>
<p>Hadoop+HBASE：<a href="https://www.cnblogs.com/lzxlfly/p/7221890.html" target="_blank" rel="noopener">https://www.cnblogs.com/lzxlfly/p/7221890.html</a></p>
<p>Hive Metastore解释：<a href="https://www.cnblogs.com/linbingdong/p/5829369.html" target="_blank" rel="noopener">https://www.cnblogs.com/linbingdong/p/5829369.html</a></p>
<p>mysql+Hive部署：<a href="https://www.cnblogs.com/kxdblog/p/4100263.html" target="_blank" rel="noopener">https://www.cnblogs.com/kxdblog/p/4100263.html</a></p>

        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        </p><p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

